<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/logo_miccall.png"/>
	<link rel="shortcut icon" href="/img/logo_miccall.png">
	
			    <title>
    Hexo
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="miccall" />
    
    	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<meta name="generator" content="Hexo 5.4.0"></head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">MICCALL</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">主页</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">分类</a>
	                    <ul class="submenu">
	                        <li>
	                        
	                    </ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="简历">
		                简历
		            </a>
		        </li>
		        
		        <li>
		            <a href="/group/" title="团队">
		                团队
		            </a>
		        </li>
		        
		        <li>
		            <a href="/gallery/" title="图库">
		                图库
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="标签">
		                标签
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/miccall" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
                    <li>
                        <a title="500px" href="http://500px.com" target="_blank" rel="noopener">
                            <i class="icon fa fa-500px"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url();background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >Recommendation system &amp; NLP</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <p>图书推荐：《构建企业级推荐系统–算法工程实现与案例分析》</p>
<h1 id="从零开始搭建推荐系统"><a href="#从零开始搭建推荐系统" class="headerlink" title="从零开始搭建推荐系统"></a>从零开始搭建推荐系统</h1><p>考虑尽量快速让新的推荐业务上线，再逐步优化提升算法效果。不能一开始就想做出一个非常完美，效果非常好的推荐系统。在没有上线之前，我们只能根据个人经验来判断算法是否有效，但是个人经验判断往往有误，另外离线评估效果好的算法在真实业务场景中的就不一定提升商业化指标，即离线评估和在线评估可能不成正相关的(即使相关，有可能相关度也非常低)。<br>早期阶段缺乏用户与数据，计算成本低所以可以用单服务器部署推荐系统。当用户足够多、单机出现困难时再考虑利用Spark等分布式或计算平台重构现有业务逻辑。</p>
<h2 id="起步阶段产品的推荐-以金融新闻资讯等文本类产品为例"><a href="#起步阶段产品的推荐-以金融新闻资讯等文本类产品为例" class="headerlink" title="起步阶段产品的推荐(以金融新闻资讯等文本类产品为例):"></a>起步阶段产品的推荐(以金融新闻资讯等文本类产品为例):</h2><p>金融新闻资讯主要是文本标的物的产品，最简单的方式是基于TF-IDF模型来构建算法，每个文本基于TF-IDF构建向量表示，通过向量的余弦相似度来计算两个文本的相似度。<br>这种文本相似度可直接用于构建相关推荐(某个文章最相似的N个文章作为相似度推荐列表)。可将用户最近看的文章的向量进行加权(根据看的时间、停留时间等)，获得用户的向量表示，用户向量与文章向量的余弦就是用户对该文章的喜好度，还可以用item-based协同过滤的计算思路(如下公式)来计算用户对新文章的评分；还有一种方式就是先对文章进行聚类然，在推荐时，以推荐用户看过的文章所在类别的其他文章作为推荐列表。<br>$$sim(u,s) = \sum score(u,s_i)\times sim(s_i,s) $$<br>新闻资讯类一般可以做成信息流推荐，基于上面的TF-IDF算法也是可以实现实时推荐的。对于新发布的一篇文章，可以基于现有的词库(corpus)来生成该文章的向量表示(如果该新文章包含某些词不在词库中，可以直接忽略这些词，虽然会使精度有所下降，但是不用对文章重新求向量化，因此可以做到实时化，向量化的过程可以每天利用所有文章作为document重新训练一次)，向量化后，这个文章就可以其他文章一样处理了。</p>
<h2 id="矩阵分解推荐算法"><a href="#矩阵分解推荐算法" class="headerlink" title="矩阵分解推荐算法"></a>矩阵分解推荐算法</h2><p><strong>核心思想</strong><br>此处主要用显式反馈(用户的真实评分)来解释矩阵分解算法。其核心思想是将用户的评分矩阵$R$分解为两个矩阵$U$和$V$的乘积；某个用户对某个item的评分，就可以用矩阵U(用户的特征向量)对应的行和矩阵V对应的列(item的特征向量)的乘积（内积）。有了用户对item的评分就很容易为ta做推荐了。</p>
<p><strong>算法原理</strong><br>将矩阵分解转化为机器学习问题，假设所有用户有评分的(u,v)组成集合为A，u为用户，v为item，通过矩阵分解将用户u和标的物v嵌入k维隐式特征空间，$p_u$,$q_v$,那么用户u对v的预测评分为$\widehat{r_uv} = p_u $真实值和预测值之间的误差为$\Deltar = r_uv - r$预测的越准，$|\Deltar|越小。所以就等价转换为求最小值的优化问题了。</p>
<p><strong>求解方法</strong><br>利用SGD,ALS来求解。 </p>
<p><strong>拓展与优化</strong></p>
<ul>
<li>整合偏差项bias；</li>
<li>增加更多用户信息输入: 具体来说可以整合用户隐式反馈（收藏，点赞，分享等）和用户人口统计学信息（年龄，性别，地域，收入等）到矩阵分解模型中。最终整合了隐式反馈和人口统计学信息（包括偏差项）的用户预测公式可以写出最终优化目标函数，同样可以用SGD和ALS求解。</li>
<li>整合时间因素:前面的模型都是静态的，用户对item的喜好和评分，以及item的受欢迎程度可能会随着时间改变，所以引入$b_u (t)$ 来表示用户偏差随着时间改变，用包含时间的$b_v (t)$表示item随着时间变化而变化的趋势。</li>
<li>整合用户对评分的置信度：防止外界因素干扰，引入user对item喜欢的概率/置信率，操作越多、时间越长、付出越大，置信度越高，增加置信度因子$c_uv$,从而得到最终的优化函数。</li>
<li>隐式反馈</li>
<li>整合用户和标的物的metadata信息，该方法可以很好解决用户和标的物的冷启动问题。搜索引擎Lyst就用此方法,一个比较好的代码实现：<a target="_blank" rel="noopener" href="http://github.com/lyst/lightfm">http://github.com/lyst/lightfm</a><h2 id="近实时矩阵分解算法："><a href="#近实时矩阵分解算法：" class="headerlink" title="近实时矩阵分解算法："></a>近实时矩阵分解算法：</h2>前面的算法都适合做批处理，通过离线处理模型，再为用户推荐。批处理适合时效性要求不高的产品，比如电商或长视频。那么对时效性高的比如抖音，网抑云等短时间产生大量标的物的产品，需要实时的矩阵分解，对矩阵进行实时化改造，可以更快反映用户兴趣变化更快整合新标的物至推荐系统。推荐论文：<br>Yanxiang Huang, Bin Cui, Jie Jiang, Kunqian Hong, Wenyu Zhang, and Yiran Xie. 2016. Real-time Video Recommendation Exploration.  <a target="_blank" rel="noopener" href="https://doi.org/10.1145/2882903.2903743">https://doi.org/10.1145/2882903.2903743</a><br>Real-time top-n recommendation in social streams. <a target="_blank" rel="noopener" href="https://www.ismll.uni-hildesheim.de/pub/pdfs/Diaz_Drumond_et_al_RECSYS2012.pdf">https://www.ismll.uni-hildesheim.de/pub/pdfs/Diaz_Drumond_et_al_RECSYS2012.pdf</a><br>Online-Updating Regularized Kernel Matrix Factorization Models for Large-Scale Recommender Systems. <a target="_blank" rel="noopener" href="https://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2008-Online_Updating_Regularized_Kernel_Matrix_Factorization_Models.pdf">https://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2008-Online_Updating_Regularized_Kernel_Matrix_Factorization_Models.pdf</a><br>Fast incremental matrix factorization for recommendation with positive-only feedback. pdf: <a target="_blank" rel="noopener" href="https://asset-pdf.scinapse.io/prod/30495595/30495595.pdf">https://asset-pdf.scinapse.io/prod/30495595/30495595.pdf</a><br>Online Personalized Recommendation Based on Streaming Implicit User Feedback. <a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007%2F978-3-319-25255-1_59#aboutcontent">https://link.springer.com/chapter/10.1007%2F978-3-319-25255-1_59#aboutcontent</a></li>
</ul>
<h2 id="FM-因子分解机-Factorization-Machines-推荐算法原理"><a href="#FM-因子分解机-Factorization-Machines-推荐算法原理" class="headerlink" title="FM: 因子分解机(Factorization Machines)推荐算法原理"></a>FM: 因子分解机(Factorization Machines)推荐算法原理</h2><p>Paper link: <a target="_blank" rel="noopener" href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf">https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf</a><br>推导简单易懂的的一篇博文：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6370127.html">https://www.cnblogs.com/pinard/p/6370127.html</a><br>矩阵分解算法我们知道是一种高效的嵌入式方法，通过将用户和标的物嵌入低维向量空间，再利用用户和标的物的嵌入向量的内积来预测user对标的物的偏好得分。而FM的核心思想就来源于矩阵分解，MF可以看做是分解机FM的特例。</p>
<h3 id="FM简单介绍"><a href="#FM简单介绍" class="headerlink" title="FM简单介绍"></a>FM简单介绍</h3><p><em>FM是推荐系统工程师应该熟练掌握和应用的必备算法</em>。最早是2010年Rendel在ICDM提出，和传统线性模型不同的是，FM考虑了特征间的交叉，可对所有特征变量交互进行建模（类似svm的核函数），特征组合对于推荐系统是非常重要的，FM就体现了这个思想（主要是二姐特征组合），因此在推荐系统和计算广告领域关注的CTR点击率和CVR转化率有良好表现。</p>
<h4 id="从LR到SVM再到FM模型"><a href="#从LR到SVM再到FM模型" class="headerlink" title="从LR到SVM再到FM模型"></a>从LR到SVM再到FM模型</h4><p>参考张俊林博士文章:推荐系统召回四模型之：全能的FM模型 - 知乎<br>LR模型是CTR预估领域早期最成功的模型，大多工业推荐排序系统采取LR这种“线性模型+人工特征组合引入非线性”的模式。因为LR模型具有简单方便易解释容易上规模等诸多好处，所以目前仍然有不少实际系统仍然采取这种模式。但是，LR模型最大的缺陷就是<strong>人工特征工程，耗时费力费人力资源</strong>,所以我们可以将特征训练阶段学习获得。其实这种二阶特征组合的使用方式，和多项式核SVM是等价的。虽然这个模型看上去貌似解决了二阶特征组合问题了，但是它有个潜在的问题：它对组合特征建模，<strong>泛化能力比较弱，尤其是在大规模稀疏特征存在的场景下</strong>，这个毛病尤其突出，比如CTR预估和推荐排序，这些场景的最大特点就是特征的大规模稀疏。所以上述模型并未在工业界广泛采用。 于是，此时FM模型优势就很明显了。 FM模型也直接引入<strong>任意两个特征的二阶特征组合</strong>，和SVM模型最大的不同，在于<strong>特征组合权重</strong>的计算方法。FM对于<strong>每个特征</strong>，学习一个大小为k的一维向量，于是，两个特征和的特征组合的权重值，通过特征<strong>对应的向量和的内积</strong>来表示。这本质上是在对<strong>特征进行embedding化表征</strong>。和目前的各种深度DNN排序模型比，它仅仅是少了2层或者3层MLP隐层，用来直接对多阶特征非线性组合建模而已，其它方面基本相同。<br>FM的这种特征embedding模式，在大规模稀疏特征应用环境下比较好用，泛化能力强的主要原因是，即使在训练数据里两个特征并未同时在训练实例里见到过次数为0，如果换做SVM的模式，是无法学会这个特征组合的权重的。但是因为FM是学习单个特征的embedding，并不依赖某个特定的特征组合是否出现过，所以只要特征和其它任意特征组合出现过(<strong>此处注意是该特征在其他任意特征组合中出现过，才可以估计，而不是单独出现，之前理解的不对</strong>)，那么就可以学习自己对应的embedding向量 。于是，尽管这个特征组合没有看到过，但是在预测的时候，如果看到这个新的特征组合，因为都能学会自己对应的embedding，所以可以通过内积算出这个新特征组合的权重。<br>其实本质上，这也是目前很多花样的embedding的最核心特点，就是从0/1这种二值硬核匹配，切换为向量软匹配，使得原先匹配不上的，现在能在一定程度上算密切程度了，具备很好的泛化性能。</p>
<h4 id="分解机参数预估与模型价值"><a href="#分解机参数预估与模型价值" class="headerlink" title="分解机参数预估与模型价值"></a>分解机参数预估与模型价值</h4><p><strong>FM在稀疏场景下的参数估计</strong><br>系数量远远小于直接在线性模型中整合二阶交叉特征的。FM的系数个数为$1+n+kn$, 而整合两两二阶交叉的线性模型系数个数为，FM的系数个数是n的线性函数而整合线性模型的系数个数是n的指数函数，当n非常大的时候，训练分解机模型在存储空间和迭代速度上是非常有优势的。<br><strong>FM的计算复杂度</strong><br>需要处理所有交叉特征，所以计算复杂度是$O(kn^2)$通过适当公式变换和数学计算可将模型复杂度降低至$O(kn)$。<br><strong>FM模型求解</strong><br>模型简单，完全可导，可用平方损失函数、logit损失函数或hinge来学习FM模型。模型参数可以用SGD或ALS等来训练<br><strong>模型预测</strong></p>
<ul>
<li>回归问题</li>
<li>二元分类问题：可通过hinge损失或logit损失来训练二元分类问题</li>
<li>排序问题： 可利用排序算法相关的损失函数来训练fm模型，利用fm来做排序学习<br>这三类问题都可以通过整合正则项到目标函数中，避免模型过拟合。</li>
</ul>
<h4 id="FM的工程实现"><a href="#FM的工程实现" class="headerlink" title="FM的工程实现"></a>FM的工程实现</h4><p>Rendel开源了一个求解FM高效C++库：libFM (<a target="_blank" rel="noopener" href="http://libfm.org/),libFM%E9%80%9A%E8%BF%87SGD">http://libfm.org/),libFM通过SGD</a>, ALS, MCMC三种方法来训练FM。<br>I Bayer提供了FM的一种实现，可以利用分解机来解决各类回归、分类、排序问题。参考论文fastFM: A Library for Factorization Machines 链接：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1505.00641.pdf">https://arxiv.org/pdf/1505.00641.pdf</a><br>从应用的角度，没必要对FM原理非常清楚，只要会用即可。libFM是方便求解FM的工具，该库在单机下运行，对于数据量大，一次无法放入内存的情况，可以利用libFM二进制的数据格式，可以更快读取，并且一次迭代只存放一批数据到内存中进行训练。<br>如果数据量非常大可以用分布式FM模型。业界开发工具推荐Angel(内置了很多FM算法及其变种的实现)，并且可以跟Spark配合使用，非常适合工业级的FM模型训练.2019.8.22Angel发布了全新3.0版本，整合了Pytorch，它在Pytorch上实现了许多算法，包括推荐领域常见算法FM, DeepFM, Wide&amp;Deep, xDeepFM, AttentionFM, DCN,和PNN等，Angel擅长推荐模型和GCN网络模型相关领域(比如社交网络分析)。</p>
<h4 id="FM的拓展"><a href="#FM的拓展" class="headerlink" title="FM的拓展"></a>FM的拓展</h4><p><strong>高阶分解机</strong><br>简单介绍：类似二阶分解机，做法没有细说。Factorization Machines: <a target="_blank" rel="noopener" href="https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/Rendle2010FM.pdf">https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/Rendle2010FM.pdf</a><br>深入介绍：发表在NIPS2016,解决了三阶甚至更高阶的特征交叉问题。Higher-Order Factorization Machines: <a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2016/file/158fc2ddd52ec2cf54d3c161f2dd6517-Paper.pdf">https://proceedings.neurips.cc/paper/2016/file/158fc2ddd52ec2cf54d3c161f2dd6517-Paper.pdf</a><br><strong>FFM</strong><br>在FM基础上提出field的概念。一般来说同一个ID类的特征（比如推荐中的user和item特征）进行one-hot而产生的所有特征都可以归为同一个field。FM的特征交叉是针对于特征之间的，FFM是针对特征与field之间的交叉。<br>细节见文献：Field-aware Factorization Machines for CTR Prediction <a target="_blank" rel="noopener" href="https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf">https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf</a><br>基于C++的FFM 开源实现：<a target="_blank" rel="noopener" href="https://github.com/ycjuan/libffm">https://github.com/ycjuan/libffm</a><br>基于Python的实现：<a target="_blank" rel="noopener" href="https://github.com/aksnzhy/xlearn">https://github.com/aksnzhy/xlearn</a> 同时包含了LR, FM, FFM等常用的机器学习模型。<br>举例说明FFM具体的运算过程：<br>Blog1：<a target="_blank" rel="noopener" href="https://blog.csdn.net/songbinxu/article/details/79662665?spm=1001.2101.3001.6650.7&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~default-7.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~default-7.no_search_link">https://blog.csdn.net/songbinxu/article/details/79662665?spm=1001.2101.3001.6650.7&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-7.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-7.no_search_link</a></p>
<p>Blog2：<a target="_blank" rel="noopener" href="https://blog.csdn.net/hiwallace/article/details/81333604?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.no_search_link">https://blog.csdn.net/hiwallace/article/details/81333604?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link</a></p>
<p>FFM开源实现之xlearn：<br>xlearn其实不止支持ffm，还支持LR和FM，三种算法对数据有所要求。LR和FM支持libsvm和csv输入格式，libffm格式会被处理成libsvm格式；FFM只支持libffm格式。<br>libffm format: label field1:feature1:value1 field2:feature2:value2<br>csv format: value_1 value_2 … value_n label<br>libvsm format: label index1:value1 index2:value2 …index_n:value_n</p>
<p><strong>DeepFM</strong><br>是17年华为诺亚方舟团队提出的将FM与DNN有效结合的模型，主要借鉴Google的Wide&amp;Deep思想进行改进，将Wide部分的LR变成FM与DNN进行特征交叉。这让DeepFM可从原始数据中学习低阶和高阶特征交叉，不需要原本WIDE&amp;DEEP需要进行复杂的人工特征工程(LR),同时训练效率更高。<br>大量应用于广告点击预估和推荐系统，美团用于CTR估计，并在文献( DeepFM: An End-to-End Wide &amp; Deep Learning Framework for CTR Prediction: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.04950.pdf">https://arxiv.org/pdf/1804.04950.pdf</a> )中对DeepFM的两种变体进行了比较；华为用于app推荐业务做了AB测试，比原来LR算法提升10%。腾讯Angel有DeepFM的实现,可尝试应用到推荐或CTR预估业务中。</p>
<h4 id="近实时FM"><a href="#近实时FM" class="headerlink" title="近实时FM"></a>近实时FM</h4><p>基于Google的FTRL的思路，可以对离线训练算法进行改造，让FM具备在线学习能力。</p>
<h4 id="FM在推荐系统上的应用"><a href="#FM在推荐系统上的应用" class="headerlink" title="FM在推荐系统上的应用"></a>FM在推荐系统上的应用</h4><p>作为一般的推荐模型可用于回归和分类，特别是在推荐系统和广告点击率预估等商业场景。<br>对于在推荐系统上的应用，可以采用回归和分类两种方法。预测评分时就是回归，预测用户是否点击item时，可看做一个分类问题，这是可以加一个logit变换，将其转化为用户点击的概率问题。<br>构建FM模型的特征主要分为4大类：</p>
<ul>
<li>用户与标的物的交汇行为信息：<br>这包括用户的点击，播放，收藏，搜索，点赞，等各种隐式行为。这些行为可以通过平展化的方式整合为特征。比如有n个用户，m个标的物，那么用户对某个标的物的行为可平展化为n+m维特子向量，特征用0或1表示，有隐式操作为1，否则为0.<br>也可将用户的每一种操作作为一个维度，每个维度的值代表用户是否操作过或者对应的操作得分(比如用户播放时长占视频总时长的比例作为得分)。</li>
<li>用户的相关信息：<br>相关信息有很多，包括人口统计学信息，如年龄，性别，职业，收入，地域，受教育程度等，还可以包括用户的行为信息，比如用户是否是会员、什么时候注册的、最后一次登录时间、最后一次付费时间等。这些信息都可以作为某一个维度的特征灌入到FM模型中。</li>
<li>标的物相关信息：<br>标的物的metadata信息都可以作为FM的特征，拿电影推荐来说，电影的评分、年代、标签、演职表、是否获奖、是否高清、地区、语言、是否付费节目等都可以作为特征。其中评分年代是数值特征，标签、演职员表示类别特征，可以采用one-hot 编码（每个电影有多个标签，对应标签上的值为1，所以这里叫做n-hot编码，而不是one-hot）。另外节目的用户行为也可以作为特征，比如节目播放次数，节目平均播放完成度等。</li>
<li>上下文信息：<br>用户在操作标的物时是包含上下文信息的，这类上下文有时间、地点、上一步操作、所在路径，甚至是天气、心情、操作系统、版本等内容。时间可以是操作时间、是否工作日、特殊日期(如双十一)等。地域对于LBS(Location Based Service，基于地理位置的服务，如美团，滴滴等)类应用非常重要。对于购物等具备漏斗行为转化的产品或业务，用户的上一步操作及所在路径对训练模型非常关键。<br>博士论文包含FM 做推荐的各种细节和方法：<br>Advanced Factorization Models for Recommender Systems Link: <a target="_blank" rel="noopener" href="https://repository.tudelft.nl/islandora/object/uuid:0b91c68f-4da7-4745-8d08-c39c0bb00e81?collection=research">https://repository.tudelft.nl/islandora/object/uuid:0b91c68f-4da7-4745-8d08-c39c0bb00e81?collection=research</a><h3 id="深度学习在推荐系统中的应用："><a href="#深度学习在推荐系统中的应用：" class="headerlink" title="深度学习在推荐系统中的应用："></a>深度学习在推荐系统中的应用：</h3>对深度学习发展历史进行了非常好的总结和梳理”On the Origin of Deep Learning“ link：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1702.07800#">https://arxiv.org/abs/1702.07800#</a><br>深度学习推荐系统综述：Deep Learning based Recommender System: A Survey and New Perspectives“ links：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.07435v4">https://arxiv.org/abs/1707.07435v4</a><br>Spotlight 是一个基于pytorch的的开源推荐算法库，提供基于分解模型和序列模型的推荐算法实现：<br><a target="_blank" rel="noopener" href="https://github.com/maciejkula/spotlight">https://github.com/maciejkula/spotlight</a><br>Facbook官方提供的基于pytorch的DLRM的深度学习的推荐模型，通过将嵌入技术、矩阵分解、分解机、MLP等技术整合起来，能够对类别特征、数值特征进行建模，学习特征之间的隐含关系：<br><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/dlrm">https://github.com/facebookresearch/dlrm</a><br>微软开源的推荐算法库recommenders，提供了多种有价值的适合工业级应用的推荐算法，包括xDeepFM、DKN、NCF,RBM,Wide&amp;Deep等(基于Pytorch)<br><a target="_blank" rel="noopener" href="https://github.com/microsoft/recommenders">https://github.com/microsoft/recommenders</a><br>除此之外还有CNTK,Theano,Gensim等比较有名的深度学习推荐算法。<br>如果公司是基于Hadoop或Sharp平台来开发推荐算法，可以研究一下Angel和Deeplearning4j，但是他们的生态不完善，资料比较少。</li>
</ul>
<h3 id="Some-FM-related-code"><a href="#Some-FM-related-code" class="headerlink" title="Some FM related code:"></a>Some FM related code:</h3><p><a target="_blank" rel="noopener" href="https://github.com/coreylynch/pyFM">https://github.com/coreylynch/pyFM</a><br>Factorization machines in python<br>857 stars</p>
<p><a target="_blank" rel="noopener" href="https://github.com/Johnson0722/CTR_Prediction">https://github.com/Johnson0722/CTR_Prediction</a><br>CTR prediction using FM FFM and DeepFM<br>718 stars</p>
<p><a target="_blank" rel="noopener" href="https://github.com/ibayer/fastFM">https://github.com/ibayer/fastFM</a><br>fastFM: A Library for Factorization Machines<br>945 stars</p>
<p><a target="_blank" rel="noopener" href="https://github.com/CastellanZhang/alphaFM">https://github.com/CastellanZhang/alphaFM</a><br>Multi-thread implementation of Factorization Machines with FTRL for binary-class classification problem.<br>666 stars</p>
<p><a target="_blank" rel="noopener" href="https://github.com/rixwew/pytorch-fm">https://github.com/rixwew/pytorch-fm</a><br>Factorization Machine models in PyTorch<br>604 stars</p>
<h3 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h3><p>Factorization machines in python<br>sklearn特征抽取API: sklearn.feature_extraction<br>字典特征抽取API: sklearn.feature_extrection.DictVectorizer<br>DictVectorizer处理对象是符号化（非数字化）、具有一定结构的数据，如字典等，将符号转化为one-hot表示。</p>
<pre><code>DictVectorizer(sparse = True,...)
    DictVectorizer.fit_transform(x)
    # x: 文本或者包含文本字符串的可迭代对象
    #返回值：返回sparse矩阵
    DictVectorizer.inverse_transform(x)
    # x: array数组或者sparse矩阵
    # 返回值：返回之前的数据格式
    DictVectorizer.get_feature_names()
    #返回值：单词列表
    DictVectorizer.transform(x)
    #按照原先的标准转换
</code></pre>
<p>使用方法：<br>1.先实例化DictVectorizer<br>2.调取方法fit_transform(x)<br>3.DictVectorizer(sparse = False, …)输出数组形式。默认值为True时，返回的是sparse矩阵。</p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Retrival O(tens of millions) to O(thousands)<br>Ranking O(thousands) to O(hundreds)<br>Post-ranking O(hundreds) to O(dozens)</p>
<p><strong>Content-based filtering &amp; collaborative filtering</strong><br>Building RS using SGD or WALS</p>
<h1 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h1><pre><code>re.search(regex, string): to check if string contains regex, return contents regex.
if sentence contains &#39;。&#39;, using string.split(&#39;。&#39;)
some special usage:
. == match any char
^aaa == match all the items start with  aaa.
aaa$ == match all the items end up with aaa.
 
</code></pre>
<h2 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h2><p>推荐知乎文章[NLP] 秒懂词向量Word2vec的本质：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26306795">https://zhuanlan.zhihu.com/p/26306795</a><br>材料阅读顺序：</p>
<ol>
<li><p>Mikolov 两篇原论文：<br>『Distributed Representations of Sentences and Documents』<br>   贡献：在前人基础上提出更精简的语言模型（language model）框架并用于生成词向量，这个框架就是 Word2vec<br>『Efficient estimation of word representations in vector space』<br>   贡献：专门讲训练 Word2vec 中的两个trick：hierarchical softmax 和 negative sampling<br>优点：Word2vec 开山之作，两篇论文均值得一读<br>缺点：只见树木，不见森林和树叶，读完不得要义。<br>   这里『森林』指 word2vec 模型的理论基础——即 以神经网络形式表示的语言模型<br>   『树叶』指具体的神经网络形式、理论推导、hierarchical softmax 的实现细节等等</p>
</li>
<li><p>北漂浪子的博客：『深度学习word2vec 笔记之基础篇』<br>缺点：太啰嗦，有点抓不住精髓</p>
</li>
<li><p>Yoav Goldberg 的论文：『word2vec Explained- Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method』<br>优点：对 negative-sampling 的公式推导非常完备<br>缺点：不够全面，而且都是公式，没有图示，略显干枯</p>
</li>
<li><p>Xin Rong 的论文：『word2vec Parameter Learning Explained』：<br>！重点推荐！<br>理论完备由浅入深非常好懂，且直击要害，既有 high-level 的 intuition 的解释，也有细节的推导过程<br>一定要看这篇paper！一定要看这篇paper！一定要看这篇paper！</p>
</li>
<li><p>来斯惟的博士论文『基于神经网络的词和文档语义向量表示方法研究』以及他的博客（网名：licstar）<br>可以作为更深入全面的扩展阅读，这里不仅仅有 word2vec，而是把词嵌入的所有主流方法通通梳理了一遍</p>
</li>
<li><p>几位大牛在知乎的回答：『word2vec 相比之前的 Word Embedding 方法好在什么地方？』<br>刘知远、邱锡鹏、李韶华等知名学者从不同角度发表对 Word2vec 的看法，非常值得一看</p>
</li>
<li><p>Sebastian 的博客：『On word embeddings - Part 2: Approximating the Softmax』<br>详细讲解了 softmax 的近似方法，Word2vec 的 hierarchical softmax 只是其中一种<br>机器之心– Word Embedding Papers | 经典再读之Word2Vec: <a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2020-06-15-6">https://www.jiqizhixin.com/articles/2020-06-15-6</a></p>
</li>
</ol>
<h1 id="知识图谱"><a href="#知识图谱" class="headerlink" title="知识图谱"></a><strong>知识图谱</strong></h1>
            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'http-miccall-tech'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'http://example.com/2021/11/29/recommendation_system/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://example.com/2021/11/29/recommendation_system/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//http-miccall-tech.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a target="_blank" rel="noopener" href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
                <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>
    </div>
</body>



 	
</html>
